{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. What is Delta Lake in Databricks?\n",
       "    a. A tool to handle data lakes.\n",
       "    b. A tool to handle data files. \n",
       "    c. The default format for all operations in Databricks.\n",
       "    d. Both b and c.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: d. Both b and c.</span>\n",
       "\n",
       "Explanation: According to the documentation, Delta Lake is the optimized storage layer that provides the foundation for tables in a lakehouse on Databricks. It is a tool that extends Parquet data files for ACID transactions and metadata handling. Also, Delta Lake is the default format for all operations on Databricks. So both options b and c are correct.\n",
       "</details>\n",
       "\n",
       "2. Which of the following is not a feature of Delta Lake?\n",
       "    a. Compatible with Apache Spark APIs.\n",
       "    b. Integrated with Structured Streaming.\n",
       "    c. Provides incremental processing at scale.\n",
       "    d. It requires manual updates to the table schema.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: d. It requires manual updates to the table schema.</span>\n",
       "\n",
       "Explanation: According to the documentation, Delta Lake actually supports automatic updates to the table schema. So, option d is not a feature of Delta Lake. Options a, b, and c are features of Delta Lake as confirmed in the documentation.\n",
       "</details>\n",
       "\n",
       "3. What's the purpose of using managed tables in Databricks?\n",
       "    a. To give direct access to the data.\n",
       "    b. To offer transactional guarantees and optimized performance.\n",
       "    c. To allow data redundancy.\n",
       "    d. To create tables using formats other than Delta Lake.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: b. To offer transactional guarantees and optimized performance.</span>\n",
       "\n",
       "Explanation: As per the documentation, managed tables on Databricks are recommended for all tabular data managed in Databricks because they offer transactional guarantees and optimized performance. Options a, c, and d are not accurate according to the mentioned documentation.\n",
       "</details>\n",
       "\n",
       "4. Which statement about external tables in Databricks is false?\n",
       "    a. They decouple the management of underlying data files from the metastore registration.\n",
       "    b. They can store data files using common formats readable by external systems.\n",
       "    c. When an external table is dropped, the underlying data in cloud storage is also deleted.\n",
       "    d. External tables are recommended when you require direct access to the data.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: c. When an external table is dropped, the underlying data in cloud storage is also deleted.</span>\n",
       "\n",
       "Explanation: According to the documentation, when an external table is dropped, the underlying data in cloud storage is not deleted. This makes option c false.\n",
       "</details>\n",
       "\n",
       "5. What does a view in Databricks refer to?\n",
       "    a. An object endorsed by Apache.\n",
       "    b. A data visualization tool.\n",
       "    c. A read-only object composed from one or more tables and views in a metastore.\n",
       "    d. A relationship pattern between tables.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: c. A read-only object composed from one or more tables and views in a metastore.</span>\n",
       "\n",
       "Explanation: As per the documentation, a view in Databricks is a read-only object composed from one or more tables and views in a metastore.\n",
       "</details>\n",
       "\n",
       "6. How long is the Delta Lake table history retained by default?\n",
       "    a. 7 days.\n",
       "    b. 30 days.\n",
       "    c. 7 months.\n",
       "    d. 1 year.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: b. 30 days.</span>\n",
       "\n",
       "Explanation: According to the official documentation, the Delta Lake table history is retained for 30 days by default.\n",
       "</details>\n",
       "\n",
       "7. What setting must be altered to retain Delta Lake table history longer than the default duration?\n",
       "    a. spark.sql.files.ignoreMissingFiles\n",
       "    b. delta.logRetentionDuration\n",
       "    c. hive.metastore.warehouse.dir\n",
       "    d. spark.sql.legacy.allowUntypedScalaUDF\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: b. delta.logRetentionDuration.</summary>\n",
       "\n",
       "Explanation: According to the documentation, in order to change the retention duration for the Delta Lake table history, the delta.logRetentionDuration property must be configured. So option b is correct.\n",
       "</details>\n",
       "\n",
       "8. In Databricks, is it possible to query a version of a table that has been removed by a VACUUM operation?\n",
       "    a. Yes, always.\n",
       "    b. Yes, but only if the version was removed within the last 7 days.\n",
       "    c. No, the VACUUM operation permanently deletes the versions.\n",
       "    d. It depends on the delta.logRetentionDuration setting.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: b. Yes, but only if the version was removed within the last 7 days.</span>\n",
       "\n",
       "Explanation: According to the documentation, if you run VACUUM daily with the default values, 7 days of data is generally available for time travel. So, you should be able to query a table version that has been removed by VACUUM, as long as it was removed within the last 7 days.\n",
       "</details>\n",
       "\n",
       "9. When a user creates a table using SQL commands, Spark, or other tools in Databricks, by default, what type of table is it?\n",
       "    a. An External table.\n",
       "    b. A Basic Table.\n",
       "    c. A Managed Table.\n",
       "    d. A Delta table.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: c. A Managed Table.</span>\n",
       "\n",
       "Explanation: As per the documentation, by default, any time a user creates a table using SQL commands, Spark, or other tools in Databricks, the table is a managed table.\n",
       "</details>\n",
       "\n",
       "10. What happens to the data files of a managed table in Databricks when the table is dropped?\n",
       "    a. The data files are not deleted.\n",
       "    b. The data files are deleted immediately.\n",
       "    c. The data files are deleted within 30 days.\n",
       "    d. The data files are archived for future access.\n",
       "\n",
       "<details>\n",
       "<summary>Answer</summary>\n",
       "<span style='color:red'>Answer: c. The data files are deleted within 30 days.</span>\n",
       "\n",
       "Explanation: According to the documentation, when a managed table is dropped"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Function to generate study questions from documentation and outline\n",
    "def generar_preguntas_sobre_documentacion(document_path, outline_path, num_preguntas=10):\n",
    "    # Read the documentation from the file\n",
    "    with open(document_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        documentacion = file.read()\n",
    "\n",
    "    # Read the outline from the file\n",
    "    with open(outline_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        outline = file.read()\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Using the following documentation and section outline, generate {num_preguntas} theoretical questions. \"\n",
    "        f\"Each question should test the understanding of key concepts, functions, or features described \"\n",
    "        f\"in the documentation, and should cover the topics listed in the section outline. The questions should include \"\n",
    "        f\"multiple-choice options formatted as a Markdown list. Additionally, include a hidden section or collapsible element \"\n",
    "        f\"that reveals the correct answer along with a detailed explanation.\\n\\n\"\n",
    "        f\"Documentation:\\n\\n{documentacion}\\n\\n\"\n",
    "        f\"Section Outline:\\n\\n{outline}\\n\\n\"\n",
    "        \"Format:\\n\"\n",
    "        \"1. Theoretical problem statement based on the documentation and outline.\\n\"\n",
    "        \"2. Multiple-choice options formatted in Markdown as a list (a,b,c,d).\\n\"\n",
    "        \"3. A hidden section or collapsible element that reveals the correct answer, provides a detailed explanation, \"\n",
    "        \"   and references the documentation and outline where applicable to support the correct answer and explain why the other \"\n",
    "        \"   options are incorrect. The word 'Answer' and the correct answer should be formatted in red using HTML \"\n",
    "        \"   (e.g., `<span style='color:red'>Answer</span>` and `<span style='color:red'>correct answer</span>`).\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # Create the chat completion request with streaming enabled\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    # Collect the response in chunks\n",
    "    response_content = \"\"\n",
    "    for chunk in stream:\n",
    "        response_content += chunk.choices[0].delta.content or \"\"\n",
    "\n",
    "    # Display the output as Markdown in Jupyter Notebook\n",
    "    display(Markdown(response_content))\n",
    "\n",
    "# Example usage\n",
    "document_path = \"/Users/bruno/dbks-gpt/dbks-gpt/documentation/section-II-data_managment_documentation.md\"\n",
    "outline_path = \"/Users/bruno/dbks-gpt/dbks-gpt/outline/section-II-data_managment_outline.txt\"\n",
    "\n",
    "# Generate and display study questions in Markdown format\n",
    "generar_preguntas_sobre_documentacion(document_path, outline_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbks-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
